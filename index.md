# Data Science Portfolio

Here are some of my best Data Science Projects. I have explored various machine-learning algorithms for different datasets. Feel free to contact me to learn more about my experience working with these projects.

***

[Examining the adaptibility of online education by students](https://github.com/Ranjan4Kumar/Adaptive_Online_Education)

<img src="images/Online.png"/>

**Skills used:** Python, Pandas, SKlearn, Matplotlib

**Project Objective:** Prediction on Student's Adaptability Level in Online Education.

**Quantifiable result:** We could predict the Adaptability Level resulting in [**45%** accuracy](https://github.com/Ranjan4Kumar/Adaptive_Online_Education/blob/main/Adaptivity_online_Edu.ipynb).

- Used Random Forest Regressor to predict the number of bikes rented in the city of Seoul
- The data had quite a few categorical variables which were encoded for use in the model
- Encoded categorical variables to numeric using Sklearn due to the presence of many string columns
- Cross Validation for validating the training data and model fit.
- Fit a Random Forest Regressor with high prediction accuracy through iteration

***

[The classification goal is to predict if the client will subscribe a term deposit](https://github.com/prativap1/logistic-regression/blob/main/LogProject.ipynb)

<img src="images/Bank.jpg? raw=true"/>

**Skills used:** Python, Pandas, SKlearn, Matplotlib

**Project Objective:** . To explore Bank's Marketing campaign and create meaningful insights from the data

**Quantifiable result:** We could Classify if the client will subscribe a term deposit resulting in [**91.5%** accuracy ](https://github.com/prativap1/logistic-regression/blob/main/LogProject.ipynb)).

- CatBoost is an open-source gradient boosting library for decision trees developed by Yandex. 
It is designed to handle categorical data efficiently and is particularly useful for working with datasets that contain a large number of categorical features. 
Some of the key features of CatBoost include:
- Encoded categorical variable to numeric
- Splitted train and test dataset and trained on the data
- Implemented SMOTE to balance the two class labels and improve the performance

***

[The classification goal is to predict if patient has paget's disease or not based on the orthopedic parameters](https://github.com/Ranjan4Kumar/Normal_Abnormal_patient_based_on_Orthopedic_parameters)

<img src="images/pagets.jpg?raw=true"/>

**Skills used:** Python, KNN, NB

**Project Objective:** To predict normal and abnormal status of patient based on orthopedic parameters using KNN and NB algorithm.
**Quantifiable result:** We could train the data with KNN algorithm  to attain a accuracy of [**85%**] (https://github.com/Ranjan4Kumar/Normal_Abnormal_patient_based_on_Orthopedic_parameters).

- Prepared the data for training
- Trained the data with KNN algorithm
- Trained the data with Naive Bayes algorithm
- Compared the values for best algorithm

***
[Determining if the clicks of mobile advertisement is fraud or genuine](https://github.com/Ranjan4Kumar/KAG_Numpy_Projetc/blob/main/Numpy_practice_1.ipynb)

<img src="images/talkingdata.jpg?raw=true"/>

**Skills used:** Python, Pandas, SKlearn, Bagging, Boosting

**Project Objectives:** To predict the probabiltity of click being genuine or fraud based on given features using boosting and bagging technique.

**Quantifiable result:** We got the accuracy of [**99%**](https://github.com/prativap1/DT-Ensemble/blob/main/DT_Ensemble.ipynb)

- Applied Random forest model
- Applied Adaboost ensemble algorithm
- Applied bagging classifier 
- Apllied gradient boostclassifier model
- Calculated the accuracy of all the models

